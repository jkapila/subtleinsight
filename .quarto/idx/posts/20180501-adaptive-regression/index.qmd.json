{"title":"Adaptive Regression","markdown":{"yaml":{"title":"Adaptive Regression","subtitle":"We recently put through our observation on Regression Problem in our research. This post is a nonformal attempt to explain it.\n","description":"If things are simple lets keep it simple. [Paper here](https://arxiv.org/abs/1805.01618)","license":"CC BY","author":[{"name":"Jitin Kapila","url":"https://www.jitinkapila.com"}],"citation":true,"date":"2018-05-01","format":{"html":{"toc":true,"code-fold":true,"html-math-method":"webtex","fig-cap-location":"bottom","cap-location":"bottom"}},"execute":{"eval":false,"echo":false,"warning":false},"title-block-banner":true,"image":"Post_Mape_Plot.png","categories":["research","code","analysis","r"],"tags":["regression","r"],"keywords":["mape","regression","r","adaptive","quantile","distribution"],"filters":["lightbox"],"lightbox":"auto"},"headingText":"Observation","containsRefs":false,"markdown":"\n\nHere I am trying to express our logic to find such **Observation**. Lets dive in.\n\n\nThere are different value estimation technique like regression analysis and time-series analysis. Everyone of us has experimented on regression using OLS ,MLE, Ridge, LASSO, Robust etc., and also might have evaluated them using RMSE (Root Mean/Median Square Error), MAD (Mean/Median Absolute Deviation), MAE (Mean / Median Absolute Error) and MAPE (Mean/Median Absolute Percentage Error), etc…\n\n\nBut all of these gives a single point estimate that what is the overall error looks like. Just a different thought!! can we be sure that this single value of MAPE or MAE? How easy it is to infer that our trained model has fitted well across the distribution of dependent variable?\n\n::: { layout=\"[30,70]\" }\nLet me give you a pretty small data-set to play with ***\"The Anscombe’s quartet\"***. This is a very famous data-set by _Francis Anscombe_. Please refer the plots below to understand the distribution of y1, y2, y3, y4. Isn’t it different?\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Anscombe%27s_quartet_3.svg/425px-Anscombe%27s_quartet_3.svg.png)\n:::\n\nWould the measure of central tendency and disportion be same for this data? I am sure none of us would believe but to our utter surprise we see all the descriptive stats are kind of same. Don’t believe me !!! Please see the results below ( Source: [Wikipedia](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)).\n\n::: { layout=\"[75,25]\" }\n\n![](Anscombe_Stats.png)\n\nAstonished !!! Don’t be. This is what has been hiding behind those numbers. And this is why we really won’t be able to cross certain performance level. \n:::\n\nUnless you change some features or even do a lot of hyper parameter tuning, your results won’t vary much. If you look at the average value of MAPE in each decile you would see an interesting pattern. Let us show you what we see that pattern. One day while working on a business problem where I was using regression on a dicussion with my senior, we deviced a different way of model diagnosis. We worked together to give this a shape and build on it.\n\n\n::: { layout=\"[20,80]\" }\n\nAs you can see it is absolutely evident that either of the side in the distribution of MAPE values is going wild!!!!!!! _Still overall MAPE is good (18%)._\n\n![](Pre_Mape_Plot.png)\n:::\n\n# Ideation\n\nWe worked together to build a different framework to address such issues on the go and ***reduce the MAPE deterioration on the edge of the distribution.***\n\n::: {layout=\"[55,45]\"}\n\n![Plot for Classical Bath Tub Curve using a Hazard Function](Image_2.png){fig-align=\"right\" width=450px height=250px}\n\nThis problems gives rise to a concept we named as ***Distribution Assertive Regression (DAR)***.\nDAR is a framework that is based on cancelling the weakness of one point summaries by using the classical concepts of Reliability Engineering : **The Bath Tub Curve**.\nThe Specialty of this curve is that it gives you the likelihood which areas one tends to have high failure rates.\n\n:::\n\nIn our experiments when we replace failure with MAPE value and the Time with sorted (ascending) value of target / dependent variable, we observe the same phenomenon. This is likely to happen because most of regression techniques assumes Normal (Gaussian) Distribution of data and fits itself towards the central tendency of this distribution.\n\nBecause of this tendency, any regression methods tends to learn less about data which are away from the central tendency of the target.\n\nLets look at BostonHousing data from `mlbench` package in R.\n\n::: {.column-body}\n![](Plot_Bathtub.png) \n:::\nHere the MAPE is calculated for each decile split of ordered target variable. As you can observe it is following the bath tub curve. Hence the validates our hypothesis that the regression method is not able to understand much about the data at the either ends of the distribution.\n\n\n<!-- {{< image classes=\"fancybox nocaption right fig-75\" <title=\"**Plot for MAPE Bath Tub Curve for Decile Split “mdev” from Data**\"> } } -->\n\n\n# Analysis\n\nNow the DAR framework essentially fixes this weakness of regression method and understands the behavior of data which is stable and can be tweak in a fashion that can be use in general practice.\n\n::: {layout=\"[90,-1,9]\" .column-page-left}\n\n![](Post_Mape_Plot.png)\n\nPlot of MAPE Bath Tub Curve after *applying DAR Framework* for Decile Split _\"mdev\"_ from Data.\n:::\n\n\nHow this framework with same method reduced MAPEs so much and made model much more stable…?? Well here it is:\n\n**The DAR framework splits the data at either ends of the order target variable and performs regression on these “split” data individually. This inherently reduces the so called “noise” part of the data and treat it as an individual data.**\n\n\nNow you might be thinking while applying regression this sounds good but how will one score this on new data. Well to answer that we used our most simple yet very effective friend “KNN” (Though any multiclass Classifier can be used here). So ideally scoring involves two step method :\n\n\n>  1) Score new value against each KNN / Multiclass Classifier model of the data  \n>  2) Based on closeness we score it with the regression method used for that part of data.\n\nSo now we know how we can improve the prediction power of data for regression. \n\nRefer flowchart below for the framework\n\n# Code and Framework\n\nHere are the some goodies.The code for the above analysis and plotting fucntions:\n<br>\n\n::: {.panel-tabset .column-body-outset}\n\n## Flow Chart of logic flow\n\n```{mermaid newdiag}\n\ngraph TB\n    \n    subgraph Testing\n        p1(Finding bucket of model to choose)\n        p1 --> p2([Making predictions <br> based on selected model for inference])\n        p2 --> p3(Consolidate final score of prediction)\n    end\n\n    subgraph Training\n        md([Fitting a <br>Regression model])==> di\n        di{Binning Data via <br/> evaluating Distribution <br/> MAPE values }\n        di --> md2([Fitting a Buckteing model <br/> to Binned MAPE Buckets])\n        md2 --> md3([Fitting Regression <br> Models on Binned Data])\n        md == Keeping main<br/>model ==> ro        \n        md3 ==> ro(Final Models <br> Binning Data Models + <br> Set of Regressoin Models)\n    end\n\n    \n    od([Data Input]) -- Training<br> Data--> md\n    od -- Testing<br> Data--> p1\n    ro -.-> p1\n    ro -.-> p2\n\n    classDef green fill:#9f6,stroke:#333,stroke-width:2px;\n    classDef yellow fill:#ff6,stroke:#333,stroke-width:2px;\n    classDef blue fill:#00f,stroke:#333,stroke-width:2px,color:#fff;\n    classDef orange fill:#f96,stroke:#333,stroke-width:4px;\n    class md,md2,md3 green\n    class di orange\n    class p1,p2 yellow\n    class ro,p3 blue\n    \n```\n\n## R - Full Code\n\n```{r main-code, eval=FALSE, echo=TRUE}\n#########################################################################\n#\n#         Implementation of Distribution Assertive Regression\n#\n#########################################################################\n\n\n# Definign Mean Average Percentage Error\nmape <- function(actual,predicted,asmean=FALSE){\n  ape <- (abs(actual-predicted) * 100)/ actual\n  ape <- if(asmean)\n    mean(ape)\n  else\n    round(ape,5)\n  ape\n}\n\ndecileBinner <- function(data,target_var,splitname,breaks = 10){\n  require(dplyr)\n  if(missing(splitname)){\n    splitname <- \"splits\"\n  }\n  data <- data %>%  \n    mutate(s = as.numeric(cut(data[,target_var],\n           breaks = breaks,rigth = T)))\n  name_ <- colnames(data)\n  name_ [name_==\"s\"] <- splitname\n  colnames(data) <- name_\n  return(data)\n}\n\n# Normalising the data\nnormalise <- function(x){\n  min_ <- min(x,na.rm = T)\n  max_ <- max(x,na.rm = T)\n  y <- (x-min_)/(max_-min_)\n  return(y)\n}\n\ndummyVar <- function(data,name,keepVar=FALSE){\n  data_ <- data.frame(data)\n  colnames(data_) <- name\n  for(t in unique(data_[,name])) {\n    new_col <- paste(name,t,sep=\"_\")\n    data_[,new_col] <- ifelse(data_[,name]==t,1,0)\n  }\n  if(keepVar){\n    data <- cbind(data,data_)\n    return(data)\n  }else{\n    return(data_)\n  }\n}\n\n# Segregating data\n# Aproach 1: KNN\n## this is to be impleted from c for calculating distance\n## and learning knn classifications\n\n## simple knn function\nknn <- function(mat, k){\n  require(fields)\n  cat('\\nGot Data :',nrow(mat),'\\n')\n  if(!is.matrix(mat)){\n    mat <- as.matrix(mat)\n  }\n  n <- nrow(mat)\n  if (n <= k) stop(\" kNN's k can not be more than nrow(data)-1! \n                   Reduce k and/or increase samples!  \")\n  neigh <- matrix(0, nrow = n, ncol = k)\n  ## This sholud be looped in chunks or implemented in C++\n  dist.mat <- fields::rdist(mat, mat)\n  # print(dist.mat)\n  for(i in 1:n) {\n    euc.dist <- dist.mat[i, ]\n    # print(euc.dist)\n    neigh[i, ] <- order(euc.dist)[2:(k + 1)]\n  }\n  \n  return(neigh)\n}\n\ndafr <- function(formula, data, model, family, dec.front=c(2),dec.back=c(2),\n                 knn.neighbours=5,...){\n  \n  dname <- paste(deparse(substitute(formula)))\n  # definign missing values\n  if (missing(model)){\n    model <- glm\n  }\n  if (missing(family)){\n    family <- \"gaussian\"\n  }\n  \n  # breaking the data set for initial split\n  # y_actual <- data[,formula$y]\n  if (!inherits(formula, \"formula\")) {\n    X <- if (is.matrix(formula$x))\n      formula$x\n    else model.matrix(terms(formula), model.frame(formula))\n    y_actual <- if (is.vector(formula$y))\n      formula$y\n    else model.response(model.frame(formula))\n    # Z <- (rownames(data) %in% cut(y_actual,breaks = 10,right = TRUE))\n  }\n  else {\n    mf <- model.frame(formula, data = data)\n    y_actual <- model.response(mf)\n    X <- model.matrix(formula, data = data)\n    # Z <- (rownames(data) %in% cut(y_actual,breaks = 10,right = TRUE))\n  }\n  \n  # making original base model\n  mod_orig <- model(formula = formula ,data = data, family = family,...=...)\n  y_orig <- predict(mod_orig,data)\n  \n  # geting mape curve\n  results <- data.frame(actuals=y_actual,original=y_orig)\n  results <- decileBinner(results,\"actuals\",splitname = \"splits\")\n  rownames(results) <- rownames(data)\n  cat('\\nActual and Prediction by Single Model :\\n')\n  print(results)\n  curve_ape <- summarize(group_by(results,splits),mape(actuals,original,asmean=TRUE))\n  curve_ape <- data.frame(curve_ape)\n  colnames(curve_ape) <- c(\"splits\",\"mape\")\n  cat('\\nSplits MAPE distributions:\\n')\n  print(curve_ape)\n  plot(curve_ape,type=\"b\",main =\" Plot of Unsplitted Absolute Percentage Error\",\n       ylab = \"Mean Absolute Percentage Error\",xlab=\"Split Index\")\n  \n  # looking at split distirbution\n  hist(results[,\"splits\"],main = \"Split Distribution\",xlab = \"Split Index\",breaks = 10)\n  cat('\\n Early Failure Region : ',dec.front,\" Wear Out Failure Region :\",dec.back,'\\n')\n  \n  # Breaking the dataset by deciles and remodelling\n  # vectorising the deciles\n  if(NROW(dec.front)==1&!is.null(dec.front)){\n    dec.front <- seq(1,dec.front)\n    cat('\\nFront Splits:',dec.front)\n    # print(results[results$splits %in% dec.front,])\n    front_idx <- row.names(results[results$splits %in% dec.front,])\n    # data_front <- data[,]\n    # print(knn(data_front,knn.neighbours))\n    cat(\"\\nFront Data has: \",\n        NROW(front_idx),\" rows\")\n  }else{\n    dec.front <- c()\n  }\n  if(NROW(dec.back)==1&!is.null(dec.back)){\n    dec.back <- seq(10,(11-dec.back))\n    cat('\\nBack Splits:',dec.back)\n    # data_back <- data[rownames(results[results$splits %in% dec.back,]),]\n    back_idx <- row.names(results[results$splits %in% dec.back,])\n    cat(\"\\nBack Data has: \",NROW(back_idx),\" rows\")\n  }else{\n    dec.back <- c()\n  }\n  \n  # data_mid <- data[rownames(results[!results$splits %in% c(dec.back,dec.front),]),]\n  mid_idx <- rownames(results[!results$splits %in% c(dec.back,dec.front),])\n  cat(\"\\nMid Data has: \",\n      NROW(mid_idx),\" rows\\nData has: \",nrow(data),\" rows\")\n  \n  # generating splitted models and calculating mapes\n  pred_dec <- c()\n  models <- list()\n  if(length(dec.front)>0){\n    \n    mod_front <- model(formula = formula ,data = data[front_idx,], family = family,...=...)\n    models$Front_Model <- mod_front\n    # cat(nrow(dataf <- data[front_idx,]))\n    models$knn.front <- knn(data[front_idx,],knn.neighbours)\n    pred_front <- predict(mod_front,data[front_idx,])\n    pred_dec <- pred_front\n  }\n  if(NROW(mid_idx)>0){\n    mod_mid <- model(formula = formula ,data = data[mid_idx,], family = family,...=...)\n    models$Mid_Model <- mod_mid\n    models$knn.mid <- knn(data[mid_idx,],knn.neighbours)\n    pred_mid <- predict(mod_mid,data[mid_idx,])\n    pred_dec <- c(pred_dec,pred_mid)\n  }\n  if(length(dec.back)>0){\n    mod_back <- model(formula = formula ,data = data[back_idx,], family = family,...=...)\n    models$Back_Model <- mod_back\n    models$knn.back <- knn(data[back_idx,],knn.neighbours)\n    pred_back <- predict(mod_back,data[back_idx,])\n    pred_dec <- c(pred_dec,pred_back)\n  }\n  cat(\"\\nDeciled Prediction has:\",NROW(pred_dec),\" value \\n\")\n  # Replotting curve of mape\n  results[,\"dec_pred\"] <- pred_dec\n  print(results)\n  curve_ape_dec <- summarize(group_by(results,splits),mape(actuals,dec_pred,asmean=TRUE))\n  curve_ape_dec <- data.frame(curve_ape_dec)\n  colnames(curve_ape_dec) <- c(\"split\",\"mape_dec\")\n  curve_ape <- cbind(curve_ape,round(curve_ape_dec[[\"mape_dec\"]],2))\n  colnames(curve_ape)[3]  <- \"mape_dec\"\n  print(curve_ape)\n  plot(curve_ape_dec,type=\"b\",main =\" Plot of Splitted Absolute Percentage Error\",\n       ylab = \"Mean Absolute Percentage Error\",xlab=\"Split Index\")\n  \n  \n  # models <- list(exists(mod_front),exists(mod_mid),exists(mod_back))\n  dafr <- list(formula = dname,models=models,results= results[,c(3,1,2,4)],\n               mapes=curve_ape,split.freq=table(results[,\"splits\"]))\n  return(dafr)\n  \n}\n```\n\n\n## R - Plotting\n\n```{r plot-code, eval=FALSE, echo=TRUE}\n\n\n\n## Testing Distribution Assertive Regression with Boston Housing Data\nlibrary(dplyr)\nlibrary(mlbench)\n\ndata(\"BostonHousing\")\ndf <- BostonHousing[order(BostonHousing[,'medv']),]\n\n# Using DAR with lm Model\nmod_dfar <- dafr(medv ~. , data = df,dec.front = 3,dec.back = 3)\nsummary(mod_dfar)\nmod_dfar$call\nmod_dfar$models\nmod_dfar$results\nmod_dfar$mapes\nmod_dfar$split.freq\n\n# Using DAR with lm Model\nmod_dfar <- dafr(medv ~. , data = df, dec.front = 3, dec.back = 3,model = glm)\n\n\n# Plotting which creates plots in the blog post\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(scales)\n\n\n# The Bath Tub Curve\nmod_dfar$mapes %>%\n  ggplot(aes(x=splits,y=mape)) +\n  geom_point(size=3) +\n  geom_line(size=1,linetype='dashed') +\n  scale_x_continuous(breaks = 1:10) +\n  theme_light() +\n  labs(title = 'Plot of Unsplitted Absolute Percentage Error',\n       x=\"Split Index\",\n       y=\"% MAPE Value\",\n       caption = \"Data Boston Housing (mlbench)\\n Model: Linear Model with Default Parameters\")\n\n\n# The shaded region with unsplitted regression\nmean_mape <- round(mean(mod_dfar$mapes$mape))\nmod_dfar$mapes %>%\n  ggplot(aes(x=splits,y=mape)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(yintercept = mean_mape, linetype=\"twodash\",size=1.1) +\n  geom_rect(aes(xmin = 1, xmax = 3.5, ymin = -Inf, ymax = Inf),\n            fill = \"red\", alpha = 0.03)+\n  geom_rect(aes(xmin = 7.5, xmax = 10, ymin = -Inf, ymax = Inf),\n            fill = \"red\", alpha = 0.03) +\n  annotate(geom = \"label\", x = 5.5, y = 20, \n           label = \"Avg MAPE : 18%\",\n           color = \"black\",size = 4)+\n  scale_x_continuous(breaks = 1:10) +\n  theme_bw() +\n  labs(title = 'Plot of Unsplitted Absolute Percentage Error',\n       subtitle = \"Shaded regions indicate MAPE with High Values\",\n       x=\"Split Index\",\n       y=\"% MAPE Value\",\n       caption = \"Data Boston Housing (mlbench)\\n Model: Linear Model with Default Parameters\")\n\n\n# The shaded region with splitted regression\nmean_mape_dafr <- round(mean(mod_dfar$mapes$mape_dec))\n\nmod_dfar$mapes %>%\n  ggplot(aes(x=splits,y=mape_dec)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(yintercept = mean_mape_dafr,linetype=\"twodash\",size=1.05) +\n  geom_rect(aes(xmin = 1, xmax = 3.5, ymin = -Inf, ymax = Inf),\n            fill = \"purple\", alpha = 0.03)+\n  geom_rect(aes(xmin = 7.5, xmax = 10, ymin = -Inf, ymax = Inf),\n            fill = \"purple\", alpha = 0.03) +\n  annotate(geom = \"label\", x = 5.5, y = 12.5, \n           label = \"Avg MAPE : 11%\",\n           color = \"black\",size = 4)+\n  scale_x_continuous(breaks = 1:10) +\n  theme_bw() +\n  labs(title = 'Plot of Splitted Absolute Percentage Error with Distributed Assertive Regression',\n       subtitle = \"Shaded regions indicate MAPE for Front Decile: 3 & Back Decile: 3\",\n       x=\"Split Index\",\n       y=\"% MAPE Value\",\n       caption = \"Data Boston Housing (mlbench)\n                  Model: Linear Model with Default Parameters\")\n\n```\n\n:::\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":false,"cache":null,"freeze":true,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"include-after-body":["../../plausible.html"],"filters":["lightbox"],"toc":true,"html-math-method":"webtex","output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.179","theme":{"light":"litera","dark":"darkly"},"title-block-banner":true,"title":"Adaptive Regression","subtitle":"We recently put through our observation on Regression Problem in our research. This post is a nonformal attempt to explain it.\n","description":"If things are simple lets keep it simple. [Paper here](https://arxiv.org/abs/1805.01618)","license":"CC BY","author":[{"name":"Jitin Kapila","url":"https://www.jitinkapila.com"}],"citation":true,"date":"2018-05-01","image":"Post_Mape_Plot.png","categories":["research","code","analysis","r"],"tags":["regression","r"],"keywords":["mape","regression","r","adaptive","quantile","distribution"],"lightbox":"auto","fig-cap-location":"bottom","cap-location":"bottom"},"extensions":{"book":{"multiFile":true}}}}}