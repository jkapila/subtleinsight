[
  {
    "objectID": "posts/20210821-flow-to-test-your-hypothesis/index.html",
    "href": "posts/20210821-flow-to-test-your-hypothesis/index.html",
    "title": "A flow to test your hypothesis in Python",
    "section": "",
    "text": "All the practioners of data science always hit one giant thing to do with data and you know it well its EDA -Exploratory Data Analysis. This word EDA1 was coined by Tukey himself in his seminal book publised in 1983. But do you think that before that EDA dosen’t exsisted ?\nWell glad you thought. Before that all were doing what is called as Hypothesis Tesing. Yes, before this the race was majorly to fit the data and make most unbiased and robust estimate. But remember one thing when you talk about Hypothesis Testing it was always and majorly would be related to RCTs (Randomized Controlled Trials) a.k.a Randomized Clinical Trials and is Gold Standard of data.\n\n\n\n\n\n\n\n“More on RCTs and ODs”\n\n\n\n\n\nNow let me now not hijack the discussion to what is RCTs and Observational Data (ODs) as it is more of Philosphical Reasoning rather than other quality of data, but essentially what we are trying to find is that can we by, using stats, identify interesting patterns in data.\nThe only thing happens wit RCT data is that we tend to believe these intresting patterns coincide with some sort of ‘Cause-Effect’ kind of relationship. But essentially due to bia nature of ODs, we certainly cant conclude this. And hence, can only find intresting patterns.\n\n\n\nLets move on. The big question is, for whatever reason you are doing HT , you are doing it for finding something intresting. And that something intresting is usually found by using Post-Hoc Tests. Now there are variety of Post-Hocs available but what is more know and hence easily found to be implemented in Tukey’s HSD.\nSo lets directly jump to how to follow this procedure. We’ll be using bioinfokit for this, as it is much simpler wrapper around whats impelmented in statsmodels."
  },
  {
    "objectID": "posts/20210821-flow-to-test-your-hypothesis/index.html#results",
    "href": "posts/20210821-flow-to-test-your-hypothesis/index.html#results",
    "title": "A flow to test your hypothesis in Python",
    "section": "Results",
    "text": "Results\nLevens Test Result:\n                 Parameter    Value\n0      Test statistics (W)  14.5856\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nBartletts Test Result:\n                 Parameter    Value\n0      Test statistics (T)  61.2143\n1  Degrees of freedom (Df)   4.0000\n2                  p value   0.0000\n\nANOVA\\ANCOVA Test Result:\n                           df     sum_sq    mean_sq         F  PR(>F)      n2\nIntercept                 1.0  6195.1701  6195.1701  296.3452  0.0000  0.2727\nC(cylinders)              4.0  7574.5864  1893.6466   90.5824  0.0000  0.3334\nC(origin)                 2.0   241.0703   120.5351    5.7658  0.0034  0.0106\nC(cylinders):C(origin)    8.0   577.4821    72.1853    3.4530  0.0046  0.0254\nResidual                389.0  8132.1404    20.9052       NaN     NaN     NaN\n\nTukey HSD Result:\n   group1  group2     Diff    Lower    Upper  q-value  p-value\n0       8       4  14.3237  12.8090  15.8383  36.6527   0.0010\n1       8       6   5.0226   3.1804   6.8648  10.5671   0.0010\n2       8       3   5.5869  -0.7990  11.9728   3.3909   0.1183\n3       8       5  12.4036   5.0643  19.7428   6.5503   0.0010\n4       4       6   9.3011   7.6765  10.9256  22.1910   0.0010\n5       4       3   8.7368   2.4102  15.0633   5.3524   0.0017\n6       4       5   1.9201  -5.3676   9.2078   1.0212   0.9000\n7       6       3   0.5643  -5.8486   6.9772   0.3410   0.9000\n8       6       5   7.3810   0.0182  14.7437   3.8854   0.0491\n9       3       5   6.8167  -2.7539  16.3873   2.7606   0.2919"
  },
  {
    "objectID": "posts/packages.html",
    "href": "posts/packages.html",
    "title": "Packages",
    "section": "",
    "text": "Grouped Trasformation\n\n\n  \nOld Work\n\nMisc Work - Lot of prior work but not well document can be found at Github\nPygemodels - A package for fitting Growth and Epidemiology Models in Python Github\nVCVN - R package for Variable Selection, Curve Fitting, Variable Conversion and Normalization Github\nSimulator Series in R - A Curve and Distribution Simulator in R Github\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{kapila2020,\n  author = {Jitin Kapila},\n  editor = {},\n  title = {Packages},\n  date = {2020-05-23},\n  url = {https://www.jitinkapila.com/posts/packages.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJitin Kapila. 2020. “Packages.” May 23, 2020. https://www.jitinkapila.com/posts/packages.html."
  },
  {
    "objectID": "posts/20220902-linear-methods-for-classification/index.html",
    "href": "posts/20220902-linear-methods-for-classification/index.html",
    "title": "Subtle Insight",
    "section": "",
    "text": "hope new work would come here\nasn dsome more text"
  },
  {
    "objectID": "posts/20180501-adaptive-regression/index.html",
    "href": "posts/20180501-adaptive-regression/index.html",
    "title": "Adaptive Regression",
    "section": "",
    "text": "Ideation\nWe worked together to build a different framework to address such issues on the go and reduce the MAPE deterioration on the edge of the distribution.\n\n\n\n\n\nPlot for Classical Bath Tub Curve using a Hazard Function\n\n\n\nThis problems gives rise to a concept we named as Distribution Assertive Regression (DAR). DAR is a framework that is based on cancelling the weakness of one point summaries by using the classical concepts of Reliability Engineering : The Bath Tub Curve. The Specialty of this curve is that it gives you the likelihood which areas one tends to have high failure rates.\n\n\n\nIn our experiments when we replace failure with MAPE value and the Time with sorted (ascending) value of target / dependent variable, we observe the same phenomenon. This is likely to happen because most of regression techniques assumes Normal (Gaussian) Distribution of data and fits itself towards the central tendency of this distribution.\nBecause of this tendency, any regression methods tends to learn less about data which are away from the central tendency of the target.\nLets look at BostonHousing data from mlbench package in R.\n\n\n\n\n\n\n\nHere the MAPE is calculated for each decile split of ordered target variable. As you can observe it is following the bath tub curve. Hence the validates our hypothesis that the regression method is not able to understand much about the data at the either ends of the distribution.\n\n\n\nAnalysis\nNow the DAR framework essentially fixes this weakness of regression method and understands the behavior of data which is stable and can be tweak in a fashion that can be use in general practice.\n\n\n\n\n\n\n\n\n \n\n\nPlot of MAPE Bath Tub Curve after applying DAR Framework for Decile Split “mdev” from Data.\n\n\n\nHow this framework with same method reduced MAPEs so much and made model much more stable…?? Well here it is:\nThe DAR framework splits the data at either ends of the order target variable and performs regression on these “split” data individually. This inherently reduces the so called “noise” part of the data and treat it as an individual data.\nNow you might be thinking while applying regression this sounds good but how will one score this on new data. Well to answer that we used our most simple yet very effective friend “KNN” (Though any multiclass Classifier can be used here). So ideally scoring involves two step method :\n\n\nScore new value against each KNN / Multiclass Classifier model of the data\n\nBased on closeness we score it with the regression method used for that part of data.\n\n\nSo now we know how we can improve the prediction power of data for regression.\nRefer flowchart below for the framework\n\n\nCode and Framework\nHere are the some goodies.The code for the above analysis and plotting fucntions: \n\nFlow Chart of logic flowR - Full CodeR - Plotting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#########################################################################\n#\n#         Implementation of Distribution Assertive Regression\n#\n#########################################################################\n\n\n# Definign Mean Average Percentage Error\nmape <- function(actual,predicted,asmean=FALSE){\n  ape <- (abs(actual-predicted) * 100)/ actual\n  ape <- if(asmean)\n    mean(ape)\n  else\n    round(ape,5)\n  ape\n}\n\ndecileBinner <- function(data,target_var,splitname,breaks = 10){\n  require(dplyr)\n  if(missing(splitname)){\n    splitname <- \"splits\"\n  }\n  data <- data %>%  \n    mutate(s = as.numeric(cut(data[,target_var],\n           breaks = breaks,rigth = T)))\n  name_ <- colnames(data)\n  name_ [name_==\"s\"] <- splitname\n  colnames(data) <- name_\n  return(data)\n}\n\n# Normalising the data\nnormalise <- function(x){\n  min_ <- min(x,na.rm = T)\n  max_ <- max(x,na.rm = T)\n  y <- (x-min_)/(max_-min_)\n  return(y)\n}\n\ndummyVar <- function(data,name,keepVar=FALSE){\n  data_ <- data.frame(data)\n  colnames(data_) <- name\n  for(t in unique(data_[,name])) {\n    new_col <- paste(name,t,sep=\"_\")\n    data_[,new_col] <- ifelse(data_[,name]==t,1,0)\n  }\n  if(keepVar){\n    data <- cbind(data,data_)\n    return(data)\n  }else{\n    return(data_)\n  }\n}\n\n# Segregating data\n# Aproach 1: KNN\n## this is to be impleted from c for calculating distance\n## and learning knn classifications\n\n## simple knn function\nknn <- function(mat, k){\n  require(fields)\n  cat('\\nGot Data :',nrow(mat),'\\n')\n  if(!is.matrix(mat)){\n    mat <- as.matrix(mat)\n  }\n  n <- nrow(mat)\n  if (n <= k) stop(\" kNN's k can not be more than nrow(data)-1! \n                   Reduce k and/or increase samples!  \")\n  neigh <- matrix(0, nrow = n, ncol = k)\n  ## This sholud be looped in chunks or implemented in C++\n  dist.mat <- fields::rdist(mat, mat)\n  # print(dist.mat)\n  for(i in 1:n) {\n    euc.dist <- dist.mat[i, ]\n    # print(euc.dist)\n    neigh[i, ] <- order(euc.dist)[2:(k + 1)]\n  }\n  \n  return(neigh)\n}\n\ndafr <- function(formula, data, model, family, dec.front=c(2),dec.back=c(2),\n                 knn.neighbours=5,...){\n  \n  dname <- paste(deparse(substitute(formula)))\n  # definign missing values\n  if (missing(model)){\n    model <- glm\n  }\n  if (missing(family)){\n    family <- \"gaussian\"\n  }\n  \n  # breaking the data set for initial split\n  # y_actual <- data[,formula$y]\n  if (!inherits(formula, \"formula\")) {\n    X <- if (is.matrix(formula$x))\n      formula$x\n    else model.matrix(terms(formula), model.frame(formula))\n    y_actual <- if (is.vector(formula$y))\n      formula$y\n    else model.response(model.frame(formula))\n    # Z <- (rownames(data) %in% cut(y_actual,breaks = 10,right = TRUE))\n  }\n  else {\n    mf <- model.frame(formula, data = data)\n    y_actual <- model.response(mf)\n    X <- model.matrix(formula, data = data)\n    # Z <- (rownames(data) %in% cut(y_actual,breaks = 10,right = TRUE))\n  }\n  \n  # making original base model\n  mod_orig <- model(formula = formula ,data = data, family = family,...=...)\n  y_orig <- predict(mod_orig,data)\n  \n  # geting mape curve\n  results <- data.frame(actuals=y_actual,original=y_orig)\n  results <- decileBinner(results,\"actuals\",splitname = \"splits\")\n  rownames(results) <- rownames(data)\n  cat('\\nActual and Prediction by Single Model :\\n')\n  print(results)\n  curve_ape <- summarize(group_by(results,splits),mape(actuals,original,asmean=TRUE))\n  curve_ape <- data.frame(curve_ape)\n  colnames(curve_ape) <- c(\"splits\",\"mape\")\n  cat('\\nSplits MAPE distributions:\\n')\n  print(curve_ape)\n  plot(curve_ape,type=\"b\",main =\" Plot of Unsplitted Absolute Percentage Error\",\n       ylab = \"Mean Absolute Percentage Error\",xlab=\"Split Index\")\n  \n  # looking at split distirbution\n  hist(results[,\"splits\"],main = \"Split Distribution\",xlab = \"Split Index\",breaks = 10)\n  cat('\\n Early Failure Region : ',dec.front,\" Wear Out Failure Region :\",dec.back,'\\n')\n  \n  # Breaking the dataset by deciles and remodelling\n  # vectorising the deciles\n  if(NROW(dec.front)==1&!is.null(dec.front)){\n    dec.front <- seq(1,dec.front)\n    cat('\\nFront Splits:',dec.front)\n    # print(results[results$splits %in% dec.front,])\n    front_idx <- row.names(results[results$splits %in% dec.front,])\n    # data_front <- data[,]\n    # print(knn(data_front,knn.neighbours))\n    cat(\"\\nFront Data has: \",\n        NROW(front_idx),\" rows\")\n  }else{\n    dec.front <- c()\n  }\n  if(NROW(dec.back)==1&!is.null(dec.back)){\n    dec.back <- seq(10,(11-dec.back))\n    cat('\\nBack Splits:',dec.back)\n    # data_back <- data[rownames(results[results$splits %in% dec.back,]),]\n    back_idx <- row.names(results[results$splits %in% dec.back,])\n    cat(\"\\nBack Data has: \",NROW(back_idx),\" rows\")\n  }else{\n    dec.back <- c()\n  }\n  \n  # data_mid <- data[rownames(results[!results$splits %in% c(dec.back,dec.front),]),]\n  mid_idx <- rownames(results[!results$splits %in% c(dec.back,dec.front),])\n  cat(\"\\nMid Data has: \",\n      NROW(mid_idx),\" rows\\nData has: \",nrow(data),\" rows\")\n  \n  # generating splitted models and calculating mapes\n  pred_dec <- c()\n  models <- list()\n  if(length(dec.front)>0){\n    \n    mod_front <- model(formula = formula ,data = data[front_idx,], family = family,...=...)\n    models$Front_Model <- mod_front\n    # cat(nrow(dataf <- data[front_idx,]))\n    models$knn.front <- knn(data[front_idx,],knn.neighbours)\n    pred_front <- predict(mod_front,data[front_idx,])\n    pred_dec <- pred_front\n  }\n  if(NROW(mid_idx)>0){\n    mod_mid <- model(formula = formula ,data = data[mid_idx,], family = family,...=...)\n    models$Mid_Model <- mod_mid\n    models$knn.mid <- knn(data[mid_idx,],knn.neighbours)\n    pred_mid <- predict(mod_mid,data[mid_idx,])\n    pred_dec <- c(pred_dec,pred_mid)\n  }\n  if(length(dec.back)>0){\n    mod_back <- model(formula = formula ,data = data[back_idx,], family = family,...=...)\n    models$Back_Model <- mod_back\n    models$knn.back <- knn(data[back_idx,],knn.neighbours)\n    pred_back <- predict(mod_back,data[back_idx,])\n    pred_dec <- c(pred_dec,pred_back)\n  }\n  cat(\"\\nDeciled Prediction has:\",NROW(pred_dec),\" value \\n\")\n  # Replotting curve of mape\n  results[,\"dec_pred\"] <- pred_dec\n  print(results)\n  curve_ape_dec <- summarize(group_by(results,splits),mape(actuals,dec_pred,asmean=TRUE))\n  curve_ape_dec <- data.frame(curve_ape_dec)\n  colnames(curve_ape_dec) <- c(\"split\",\"mape_dec\")\n  curve_ape <- cbind(curve_ape,round(curve_ape_dec[[\"mape_dec\"]],2))\n  colnames(curve_ape)[3]  <- \"mape_dec\"\n  print(curve_ape)\n  plot(curve_ape_dec,type=\"b\",main =\" Plot of Splitted Absolute Percentage Error\",\n       ylab = \"Mean Absolute Percentage Error\",xlab=\"Split Index\")\n  \n  \n  # models <- list(exists(mod_front),exists(mod_mid),exists(mod_back))\n  dafr <- list(formula = dname,models=models,results= results[,c(3,1,2,4)],\n               mapes=curve_ape,split.freq=table(results[,\"splits\"]))\n  return(dafr)\n  \n}\n\n\n\n\n\n\nCode\n## Testing Distribution Assertive Regression with Boston Housing Data\nlibrary(dplyr)\nlibrary(mlbench)\n\ndata(\"BostonHousing\")\ndf <- BostonHousing[order(BostonHousing[,'medv']),]\n\n# Using DAR with lm Model\nmod_dfar <- dafr(medv ~. , data = df,dec.front = 3,dec.back = 3)\nsummary(mod_dfar)\nmod_dfar$call\nmod_dfar$models\nmod_dfar$results\nmod_dfar$mapes\nmod_dfar$split.freq\n\n# Using DAR with lm Model\nmod_dfar <- dafr(medv ~. , data = df, dec.front = 3, dec.back = 3,model = glm)\n\n\n# Plotting which creates plots in the blog post\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(scales)\n\n\n# The Bath Tub Curve\nmod_dfar$mapes %>%\n  ggplot(aes(x=splits,y=mape)) +\n  geom_point(size=3) +\n  geom_line(size=1,linetype='dashed') +\n  scale_x_continuous(breaks = 1:10) +\n  theme_light() +\n  labs(title = 'Plot of Unsplitted Absolute Percentage Error',\n       x=\"Split Index\",\n       y=\"% MAPE Value\",\n       caption = \"Data Boston Housing (mlbench)\\n Model: Linear Model with Default Parameters\")\n\n\n# The shaded region with unsplitted regression\nmean_mape <- round(mean(mod_dfar$mapes$mape))\nmod_dfar$mapes %>%\n  ggplot(aes(x=splits,y=mape)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(yintercept = mean_mape, linetype=\"twodash\",size=1.1) +\n  geom_rect(aes(xmin = 1, xmax = 3.5, ymin = -Inf, ymax = Inf),\n            fill = \"red\", alpha = 0.03)+\n  geom_rect(aes(xmin = 7.5, xmax = 10, ymin = -Inf, ymax = Inf),\n            fill = \"red\", alpha = 0.03) +\n  annotate(geom = \"label\", x = 5.5, y = 20, \n           label = \"Avg MAPE : 18%\",\n           color = \"black\",size = 4)+\n  scale_x_continuous(breaks = 1:10) +\n  theme_bw() +\n  labs(title = 'Plot of Unsplitted Absolute Percentage Error',\n       subtitle = \"Shaded regions indicate MAPE with High Values\",\n       x=\"Split Index\",\n       y=\"% MAPE Value\",\n       caption = \"Data Boston Housing (mlbench)\\n Model: Linear Model with Default Parameters\")\n\n\n# The shaded region with splitted regression\nmean_mape_dafr <- round(mean(mod_dfar$mapes$mape_dec))\n\nmod_dfar$mapes %>%\n  ggplot(aes(x=splits,y=mape_dec)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(yintercept = mean_mape_dafr,linetype=\"twodash\",size=1.05) +\n  geom_rect(aes(xmin = 1, xmax = 3.5, ymin = -Inf, ymax = Inf),\n            fill = \"purple\", alpha = 0.03)+\n  geom_rect(aes(xmin = 7.5, xmax = 10, ymin = -Inf, ymax = Inf),\n            fill = \"purple\", alpha = 0.03) +\n  annotate(geom = \"label\", x = 5.5, y = 12.5, \n           label = \"Avg MAPE : 11%\",\n           color = \"black\",size = 4)+\n  scale_x_continuous(breaks = 1:10) +\n  theme_bw() +\n  labs(title = 'Plot of Splitted Absolute Percentage Error with Distributed Assertive Regression',\n       subtitle = \"Shaded regions indicate MAPE for Front Decile: 3 & Back Decile: 3\",\n       x=\"Split Index\",\n       y=\"% MAPE Value\",\n       caption = \"Data Boston Housing (mlbench)\n                  Model: Linear Model with Default Parameters\")\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{kapila2018,\n  author = {Jitin Kapila},\n  editor = {},\n  title = {Adaptive {Regression}},\n  date = {2018-05-01},\n  url = {https://www.jitinkapila.com/posts/20180501-adaptive-regression},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJitin Kapila. 2018. “Adaptive Regression.” May 1, 2018. https://www.jitinkapila.com/posts/20180501-adaptive-regression."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I have graduated as a Mechanical Engineer, completed my Post Grads in Thermal Engineering which eventually took a twisted turn to make me Dive into Data Science. Wiht Masters in Data Science & Engg I am presently working with Nagarro Services Pvt. Ltd., as Data Science Architect, where I am responsible to create End-to-End analytical solutions that help The Business make Informed Decisions.\nI have worked on various projects from Retail Analytics to Fault Detection to Forecasting to Optimization. Over the years my understanding of various concepts in Machine Learning and Deep Learning has gained those Eyes of Observations. This blog is my attempt to make those concepts more intuitive and easy, yet it would help you to form the basis of The Work we do that creates an Impact.\nTill next time, stay observant, stay crazy, stay safe, stay healthy."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Subtle Insight",
    "section": "",
    "text": "This is normal qmd file\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nA flow to test your hypothesis in Python\n\n\nMaking life easy to do some serious hypothesis testing in python.\n\n\n\n\neda\n\n\nhypothesis\n\n\nanalysis\n\n\npython\n\n\n\n\nA simple code to run your hypothesis test.\n\n\n\n\n\n\nAug 10, 2021\n\n\nJitin Kapila\n\n\n\n\n\n\n  \n\n\n\n\nPackages\n\n\n\n\n\n\n\nportfolio\n\n\ndevelopment\n\n\n\n\nSome off the shelf work\n\n\n\n\n\n\nMay 23, 2020\n\n\nJitin Kapila\n\n\n\n\n\n\n  \n\n\n\n\nAdaptive Regression\n\n\nWe recently put through our observation on Regression Problem in our research. This post is a nonformal attempt to explain it.\n\n\n\n\nresearch\n\n\ncode\n\n\nanalysis\n\n\nr\n\n\n\n\nIf things are simple lets keep it simple. Paper here\n\n\n\n\n\n\nMay 1, 2018\n\n\nJitin Kapila\n\n\n\n\n\n\nNo matching items"
  }
]